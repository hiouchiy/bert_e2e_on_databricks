# Databricks notebook source
# MAGIC %md
# MAGIC # Hugging Face „Éà„É©„É≥„Çπ„Éï„Ç©„Éº„Éû„Éº„Åß„ÉÜ„Ç≠„Çπ„ÉàÂàÜÈ°û„É¢„Éá„É´„Çí„ÉÅ„É•„Éº„Éã„É≥„Ç∞„Åô„Çã
# MAGIC „Åì„ÅÆ„Éé„Éº„Éà„Éñ„ÉÉ„ÇØ„ÅØ„ÄÅ"cl-tohoku/bert-base-japanese-whole-word-masking"„Çí„Éô„Éº„Çπ„É¢„Éá„É´„Å®„Åó„ÅüË®ò‰∫ãÂàÜÈ°ûÂô®„Çí„Ç∑„É≥„Ç∞„É´GPU„Éû„Ç∑„É≥„ÅßÂ≠¶Áøí„Åó„Åæ„Åô„ÄÇ
# MAGIC Transformers](https://huggingface.co/docs/transformers/index)„É©„Ç§„Éñ„É©„É™„Çí‰Ωø„ÅÑ„Åæ„Åô„ÄÇ
# MAGIC
# MAGIC „Åæ„ÅöÂ∞è„Åï„Å™„Éá„Éº„Çø„Çª„ÉÉ„Éà„Çí„ÉÄ„Ç¶„É≥„É≠„Éº„Éâ„Åó„ÄÅSpark DataFrame„Å´Â§âÊèõ„Åó„ÄÅUnity Catalog„Å∏Delta Table„Å®„Åó„Å¶‰øùÂ≠ò„Åó„Åæ„Åô„ÄÇ„Éà„Éº„ÇØ„É≥Âåñ„Åæ„Åß„ÅÆÂâçÂá¶ÁêÜ„ÅØSpark‰∏ä„ÅßË°å„Çè„Çå„Åæ„Åô„ÄÇDBFS„ÅØ„ÄÅ„Éâ„É©„Ç§„Éê‰∏ä„ÅÆ„É≠„Éº„Ç´„É´„Éï„Ç°„Ç§„É´„Å®„Åó„Å¶„Éá„Éº„Çø„Çª„ÉÉ„Éà„Å´Áõ¥Êé•„Ç¢„ÇØ„Çª„Çπ„Åô„Çã„Åü„ÇÅ„ÅÆ‰æøÂÆú„Å®„Åó„Å¶‰ΩøÁî®„Åï„Çå„Å¶„Åæ„Åô„Åå„ÄÅDBFS„Çí‰ΩøÁî®„Åó„Å™„ÅÑ„Çà„ÅÜ„Å´Â§âÊõ¥„Åô„Çã„Åì„Å®„ÇÇ„Åß„Åç„Åæ„Åô„ÄÇ
# MAGIC
# MAGIC Ë®ò‰∫ã„ÅÆ„ÉÜ„Ç≠„Çπ„Éà„Éà„Éº„ÇØ„Éä„Ç§„Çº„Éº„Ç∑„Éß„É≥„ÅØ„ÄÅ„Éô„Éº„Çπ„É¢„Éá„É´„Å®„ÅÆ„Éà„Éº„ÇØ„É≥Âåñ„ÅÆ‰∏ÄË≤´ÊÄß„ÇíÊåÅ„Åü„Åõ„Çã„Åü„ÇÅ„Å´„ÄÅ„É¢„Éá„É´„ÅÆ„Éá„Éï„Ç©„É´„Éà„Éà„Éº„ÇØ„Éä„Ç§„Ç∂„Éº„ÅÆ`transformers`„ÅßË°å„Çè„Çå„Åæ„Åô„ÄÇ„Åì„ÅÆ„Éé„Éº„Éà„Éñ„ÉÉ„ÇØ„ÅØ„É¢„Éá„É´„Çí„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„Åô„Çã„Åü„ÇÅ„Å´ `transformers` „É©„Ç§„Éñ„É©„É™„ÅÆ [Trainer](https://huggingface.co/docs/transformers/main_classes/trainer) „É¶„Éº„ÉÜ„Ç£„É™„ÉÜ„Ç£„Çí‰ΩøÁî®„Åó„Åæ„Åô„ÄÇ„Åì„ÅÆ„Éé„Éº„Éà„Éñ„ÉÉ„ÇØ„ÅØ„Éà„Éº„ÇØ„Éä„Ç§„Ç∂„Éº„Å®Â≠¶ÁøíÊ∏à„Åø„É¢„Éá„É´„ÇíTransformers„ÅÆ`„Éë„Ç§„Éó„É©„Ç§„É≥`„Å´„É©„ÉÉ„Éó„Åó„ÄÅ„Éë„Ç§„Éó„É©„Ç§„É≥„ÇíMLflow„É¢„Éá„É´„Å®„Åó„Å¶„É≠„Ç∞„Å´Ë®òÈå≤„Åó„Åæ„Åô„ÄÇ
# MAGIC „Åì„Çå„Å´„Çà„Çä„ÄÅ„Éë„Ç§„Éó„É©„Ç§„É≥„Çí Spark DataFrame „ÅÆÊñáÂ≠óÂàó„Ç´„É©„É†„ÅÆ UDF „Å®„Åó„Å¶Áõ¥Êé•ÈÅ©Áî®„Åô„Çã„Åì„Å®„ÅåÁ∞°Âçò„Å´„Å™„Çä„Åæ„Åô„ÄÇ
# MAGIC
# MAGIC ## „ÇØ„É©„Çπ„Çø„ÅÆ„Çª„ÉÉ„Éà„Ç¢„ÉÉ„Éó
# MAGIC „Åì„ÅÆ„Éé„Éº„Éà„Éñ„ÉÉ„ÇØ„Åß„ÅØ„ÄÅAWS „ÅÆ `g4dn.xlarge` „ÇÑ Azure „ÅÆ `Standard_NC4as_T4_v3` „ÅÆ„Çà„ÅÜ„Å™„Ç∑„É≥„Ç∞„É´ GPU „ÇØ„É©„Çπ„Çø„ÇíÊé®Â•®„Åó„Åæ„Åô„ÄÇ„Ç∑„É≥„Ç∞„É´„Éû„Ç∑„É≥„ÇØ„É©„Çπ„Çø„ÅÆ‰ΩúÊàê](https://docs.databricks.com/clusters/configure.html) „ÅØ„ÄÅ„Éë„Éº„ÇΩ„Éä„É´„Ç≥„É≥„Éî„É•„Éº„Éà„Éù„É™„Ç∑„Éº„Çí‰ΩøÁî®„Åô„Çã„Åã„ÄÅ„ÇØ„É©„Çπ„Çø‰ΩúÊàêÊôÇ„Å´ "Single Node" „ÇíÈÅ∏Êäû„Åô„Çã„Åì„Å®„ÅßÂèØËÉΩ„Åß„Åô„ÄÇ„Åì„ÅÆ„Éé„Éº„Éà„Éñ„ÉÉ„ÇØ„ÅØDatabricks Runtime ML GPU„Éê„Éº„Ç∏„Éß„É≥14.3 LTS„ÅßÂãï‰ΩúÁ¢∫Ë™ç„Åó„Å¶„Åä„Çä„Åæ„Åô„ÄÇ
# MAGIC
# MAGIC Databricks Runtime ML „Å´„ÅØ `transformers` „É©„Ç§„Éñ„É©„É™„Åå„Éá„Éï„Ç©„É´„Éà„Åß„Ç§„É≥„Çπ„Éà„Éº„É´„Åï„Çå„Å¶„ÅÑ„Åæ„Åô„ÄÇ„Åì„ÅÆ„Éé„Éº„Éà„Éñ„ÉÉ„ÇØ„Åß„ÅØ„ÄÅ[ü§ó&nbsp;Datasets](https://huggingface.co/docs/datasets/index) „Å® [ü§ó&nbsp;Evaluate](https://huggingface.co/docs/evakyate/index)„ÇÇÂøÖË¶Å„Åß„ÄÅ„Åì„Çå„Çâ„ÅØ `%pip` „Çí‰Ωø„Å£„Å¶„Ç§„É≥„Çπ„Éà„Éº„É´„Åß„Åç„Åæ„Åô„ÄÇ

# COMMAND ----------

# MAGIC %md
# MAGIC # 0. „É©„Ç§„Éñ„É©„É™„ÅÆ„Ç§„É≥„Çπ„Éà„Éº„É´„Å®„Ç§„É≥„Éù„Éº„Éà

# COMMAND ----------

# MAGIC %pip install fugashi unidic-lite accelerate
# MAGIC dbutils.library.restartPython()

# COMMAND ----------

# DBTITLE 1,„Éë„É©„É°„Éº„Çø„Éº„ÅÆË®≠ÂÆö
################################################
# Â§âÊõ¥‰∏çË¶Å„Å™„Éë„É©„É°„Éº„Çø„Éº
################################################

# HuggingFace „É¢„Éá„É´Âêç
base_model = "tohoku-nlp/bert-base-japanese-v3"

# BERT„Å´ÊäïÂÖ•„Åô„ÇãÊñáÁ´†„ÅÆÊúÄÂ§ßÈï∑
max_length = 128

# „Éá„Éº„Çø„Å™„Å©„ÇíÊ†ºÁ¥ç„Åô„Çã„Éë„Çπ
# „Åì„ÅÆ„Éá„É¢„Åß„ÅØ„Éâ„É©„Ç§„Éê„Éº„Éé„Éº„Éâ„ÅÆ„É≠„Éº„Ç´„É´„Éá„Ç£„Çπ„ÇØ„Å´Ê†ºÁ¥ç„Åó„Åæ„Åô„Åå„ÄÅ‰ª•‰∏ã„ÅÆ„Éâ„Ç≠„É•„É°„É≥„Éà„Å´Ââá„ÇäDBFS„Å™„Å©‰ªñ„ÅÆÈÅ©Âàá„Å™Â†¥ÊâÄ„Å´Ê†ºÁ¥ç„ÇÇÂèØËÉΩ„Åß„Åô
# https://docs.databricks.com/ja/files/write-data.html
tutorial_path = "/databricks/driver" # 
import os
os.environ['TUTORIAL_PATH']=tutorial_path # Âæå„Åª„Å©Shell„Ç≥„Éû„É≥„Éâ„Åã„Çâ„Ç¢„ÇØ„Çª„Çπ„Åô„Çã„Åü„ÇÅÁí∞Â¢ÉÂ§âÊï∞„Å´„Çª„ÉÉ„Éà

# MLFlow Tracking„Å´Ë®òÈå≤„Åï„Çå„Å¶„ÅÑ„Çã„É¢„Éá„É´ÂêçÔºà„Ç¢„Éº„ÉÜ„Ç£„Éï„Ç°„ÇØ„ÉàÂêçÔºâ
model_artifact_path = "bert_model_ja"


################################################
# „É¶„Éº„Ç∂„ÉºÊØé„Å´Â§âÊõ¥„ÅåÂøÖË¶Å„Å™„Éë„É©„É°„Éº„Çø„Éº
################################################

# Unity Catalog „Ç´„Çø„É≠„Ç∞Âêç
catalog_name = "YOUR_CATALOG_NAME"

# Unity Catalog „Çπ„Ç≠„Éº„ÉûÂêç
schema_name = "YOUR_SCHEMA_NAME"

# Unity Catalog „ÉÜ„Éº„Éñ„É´Âêç
table_name = "YOUR_TABLE_NAME"

# „Éá„Éº„Çø„ÇíÊ†ºÁ¥ç„Åô„Çã„Åü„ÇÅ„ÅÆ„ÉÜ„Éº„Éñ„É´ÂêçÔºàcatalog_name.schema_name.table_nameÔºâ
table_name_full_path = f"{catalog_name}.{schema_name}.{table_name}"

# COMMAND ----------

# MAGIC %md
# MAGIC # 1. BERT„ÅÆ„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞

# COMMAND ----------

# MAGIC %md
# MAGIC ## 1-1. „Éá„Éº„Çø„ÅÆ„ÉÄ„Ç¶„É≥„É≠„Éº„Éâ„Å®„É≠„Éº„Éâ
# MAGIC „Åæ„Åö„Éá„Éº„Çø„Çª„ÉÉ„Éà„Çí„ÉÄ„Ç¶„É≥„É≠„Éº„Éâ„Åó„Åæ„Åô„ÄÇ„Åù„ÅÆÂæå„ÄÅSpark DataFrame„Å®„Åó„Å¶„É≠„Éº„Éâ„Åó„ÄÅDelta Table„Å®„Åó„Å¶Unity Catalog„Å∏‰øùÂ≠ò„Åó„Åæ„Åô„ÄÇ
# MAGIC
# MAGIC ‰ªäÂõûÁî®„ÅÑ„Çã„Éá„Éº„Çø„Çª„ÉÉ„Éà[livedoor „Éã„É•„Éº„Çπ„Ç≥„Éº„Éë„Çπ](https://www.rondhuit.com/download/ldcc-20140209.tar.gz)„ÅØ„ÄÅ[Rondhuit„ÅÆ„Çµ„Ç§„Éà](https://www.rondhuit.com/download.html)„Åã„ÇâÂÖ•Êâã„Åß„Åç„Åæ„Åô„ÄÇ

# COMMAND ----------

# DBTITLE 1,„Éá„Éº„Çø„ÅÆ„ÉÄ„Ç¶„É≥„É≠„Éº„ÉâÔºÜËß£ÂáçÔºà„Ç¢„Çø„ÉÉ„ÉÅ„Åï„Çå„ÅüVM„ÅÆ„É≠„Éº„Ç´„É´„Éá„Ç£„Çπ„ÇØ„Å´‰∏ÄÊôÇ‰øùÂ≠òÔºâ
# MAGIC %sh
# MAGIC cd $TUTORIAL_PATH
# MAGIC
# MAGIC # „Éá„Éº„Çø„ÅÆ„ÉÄ„Ç¶„É≥„É≠„Éº„Éâ
# MAGIC wget https://www.rondhuit.com/download/ldcc-20140209.tar.gz
# MAGIC
# MAGIC # „Éï„Ç°„Ç§„É´„ÅÆËß£Âáç
# MAGIC tar -zxf ldcc-20140209.tar.gz 

# COMMAND ----------

# DBTITLE 1,./text‰ª•‰∏ã„Å´„Åù„Çå„Åû„Çå„ÅÆ„É°„Éá„Ç£„Ç¢„Å´ÂØæÂøú„Åô„Çã„Éï„Ç©„É´„ÉÄ„Åå„ÅÇ„Çã„ÅÆ„ÇíÁ¢∫Ë™ç
# MAGIC %sh
# MAGIC ls $TUTORIAL_PATH/text

# COMMAND ----------

# DBTITLE 1,ÈÅ©ÂΩì„Å™„Éï„Ç°„Ç§„É´„ÅÆÂÜÖÂÆπ„ÇíÁ¢∫Ë™çÔºàÊúÄÂàù„ÅÆ3Ë°å„ÅØ„É°„Çø„Éá„Éº„Çø„Åß„ÄÅ4Ë°åÁõÆ„Åã„ÇâÊú¨ÊñáÔºâ
# MAGIC %sh
# MAGIC cat $TUTORIAL_PATH/text/it-life-hack/it-life-hack-6342280.txt

# COMMAND ----------

# DBTITLE 1,„Éï„Ç°„Ç§„É´Âêç„Å®„ÇØ„É©„Çπ„ÅÆ‰∏ÄË¶ß„ÇíPandas Dataframe„Å®„Åó„Å¶‰ΩúÊàê
import glob
import pandas as pd

# „Ç´„ÉÜ„Ç¥„É™„Éº„ÅÆ„É™„Çπ„Éà
category_list = [
    'dokujo-tsushin',
    'it-life-hack',
    'kaden-channel',
    'livedoor-homme',
    'movie-enter',
    'peachy',
    'smax',
    'sports-watch',
    'topic-news'
]

# ÂêÑ„Éá„Éº„Çø„ÅÆÂΩ¢Âºè„ÇíÊï¥„Åà„Çã
columns = ['label', 'label_name', 'file_path']
dataset_label_text = pd.DataFrame(columns=columns)
id2label = {} 
label2id = {}
for label, category in enumerate(category_list):
  
  file_names_list = sorted(glob.glob(f'{tutorial_path}/text/{category}/{category}*'))#ÂØæË±°„É°„Éá„Ç£„Ç¢„ÅÆË®ò‰∫ã„Åå‰øùÂ≠ò„Åï„Çå„Å¶„ÅÑ„Çã„Éï„Ç°„Ç§„É´„ÅÆlist„ÇíÂèñÂæó„Åó„Åæ„Åô„ÄÇ
  print(f"{category}„ÅÆË®ò‰∫ã„ÇíÂá¶ÁêÜ„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ„ÄÄ{category}„Å´ÂØæÂøú„Åô„ÇãÁï™Âè∑„ÅØ{label}„Åß„ÄÅ„Éá„Éº„ÇøÂÄãÊï∞„ÅØ{len(file_names_list)}„Åß„Åô„ÄÇ")

  id2label[label] = category
  label2id[category] = label
  
  for file in file_names_list:
      list = [[label, category, file]]
      df_append = pd.DataFrame(data=list, columns=columns)
      dataset_label_text = pd.concat([dataset_label_text, df_append], ignore_index=True, axis=0)

dataset_label_text.head()

# COMMAND ----------

# DBTITLE 1,„Éï„Ç°„Ç§„É´„ÅÆÂÜÖÂÆπ„Çí„É≠„Éº„Éâ„Åô„ÇãÈñ¢Êï∞ÔºàPandas UDFÔºâ
from pyspark.sql.functions import pandas_udf
from pyspark.sql.types import StringType

@pandas_udf(StringType())
def read_text(paths: pd.Series) -> pd.Series:

  all_text = []
  for index, file in paths.items():             #ÂèñÂæó„Åó„Åülist„Å´Âæì„Å£„Å¶ÂÆüÈöõ„ÅÆFile„Åã„Çâ„Éá„Éº„Çø„ÇíÂèñÂæó„Åó„Åæ„Åô„ÄÇ
      lines = open(file).read().splitlines()
      text = '\n'.join(lines[3:])               # „Éï„Ç°„Ç§„É´„ÅÆ4Ë°åÁõÆ„Åã„Çâ„ÇíÊäú„ÅçÂá∫„Åô„ÄÇ
      all_text.append(text)

  return pd.Series(all_text)

# COMMAND ----------

# DBTITLE 1,Pandas Dataframe„Åã„ÇâSpark Dataframe„Å´Â§âÊèõ„Åó„ÄÅ„Éï„Ç°„Ç§„É´„ÅÆÂÜÖÂÆπ„ÇíÊñ∞Ë¶èÂàó„Å®„Åó„Å¶ËøΩÂä†
from pyspark.sql.functions import col

dataset_df = spark.createDataFrame(dataset_label_text)
dataset_df = dataset_df.withColumn('text', read_text(col('file_path')))
display(dataset_df.head(5))

# COMMAND ----------

# DBTITLE 1,Ê¨°„ÅÆ„Çª„É´ÔºàSQLÔºâ„Å´Ê∏°„Åô„Åü„ÇÅ„Å´ÂøÖË¶Å„Å™Â§âÊï∞„Çí„Ç∑„Çπ„ÉÜ„É†Â§âÊï∞„Å®„Åó„Å¶„Çª„ÉÉ„Éà
spark.conf.set("my.catalogName", catalog_name)
spark.conf.set("my.schemaName", schema_name)

# COMMAND ----------

# DBTITLE 1,Unity Catalog„Å´„Ç´„Çø„É≠„Ç∞„Å®„Çπ„Ç≠„Éº„Éû„ÇíÁÑ°„Åë„Çå„Å∞‰Ωú„Çã
# MAGIC %sql
# MAGIC CREATE CATALOG IF NOT EXISTS ${my.catalogName};
# MAGIC USE CATALOG ${my.catalogName};
# MAGIC CREATE SCHEMA IF NOT EXISTS ${my.catalogName}.${my.schemaName};
# MAGIC USE SCHEMA ${my.schemaName};

# COMMAND ----------

# DBTITLE 1,Spark Dataframe„ÇíDelta Table„Å®„Åó„Å¶Unity Catalog„Å∏‰øùÂ≠ò
dataset_df.write.mode("overwrite").saveAsTable(table_name_full_path)

# COMMAND ----------

# MAGIC %md
# MAGIC ## 1-2. „Éá„Éº„ÇøÂä†Â∑•
# MAGIC
# MAGIC Spark Dataframe„Åã„ÇâHugging Face Datasets„Å∏Â§âÊèõ„Çí„Åó„ÄÅtextÂàóÔºàÊñáÁ´†„Éá„Éº„ÇøÔºâ„ÅÆÂÜÖÂÆπ„Çí„Éà„Éº„ÇØ„É≥Âåñ„Åó„Å¶„ÄÅÊñ∞„Åü„Å™Âàó„Å®„Åó„Å¶ËøΩÂä†„Åó„Åæ„Åô„ÄÇ
# MAGIC
# MAGIC Hugging Face „ÅÆ `datasets` „ÅØ `datasets.Dataset.from_spark` „Çí‰Ωø„Å£„Å¶ Spark DataFrames „Åã„Çâ„ÅÆ„É≠„Éº„Éâ„Çí„Çµ„Éù„Éº„Éà„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ[from_spark()](https://huggingface.co/docs/datasets/use_with_spark) „É°„ÇΩ„ÉÉ„Éâ„ÅÆË©≥Á¥∞„Å´„Å§„ÅÑ„Å¶„ÅØ Hugging Face „ÅÆ„Éâ„Ç≠„É•„É°„É≥„Éà„ÇíÂèÇÁÖß„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ„Åæ„Åü„ÄÅDatabricks„ÅÆ[„Åì„Å°„Çâ„ÅÆ„Éâ„Ç≠„É•„É°„É≥„Éà](https://docs.databricks.com/ja/machine-learning/train-model/huggingface/load-data.html)„ÇÇÂèÇËÄÉ„Å´„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ
# MAGIC
# MAGIC Dataset.from_spark„ÅØ„Éá„Éº„Çø„Çª„ÉÉ„Éà„Çí„Ç≠„É£„ÉÉ„Ç∑„É•„Åó„Åæ„Åô„ÄÇ„Åì„ÅÆ‰æã„Åß„ÅØ„ÄÅ„É¢„Éá„É´„ÅØ„Éâ„É©„Ç§„Éê‰∏ä„ÅßÂ≠¶Áøí„Åï„Çå„ÄÅ„Ç≠„É£„ÉÉ„Ç∑„É•„Åï„Çå„Åü„Éá„Éº„Çø„ÅØSpark„Çí‰ΩøÁî®„Åó„Å¶‰∏¶ÂàóÂåñ„Åï„Çå„Çã„Åü„ÇÅ„ÄÅ`cache_dir`„ÅØ„Éâ„É©„Ç§„Éê„Å®„Åô„Åπ„Å¶„ÅÆ„ÉØ„Éº„Ç´„Éº„Åã„Çâ„Ç¢„ÇØ„Çª„ÇπÂèØËÉΩ„Åß„Å™„Åë„Çå„Å∞„Å™„Çä„Åæ„Åõ„Çì„ÄÇDatabricks File System (DBFS)„ÅÆ„É´„Éº„Éà([AWS](https://docs.databricks.com/dbfs/index.html#what-is-the-dbfs-root)|[Azure](https://learn.microsoft.com/azure/databricks/dbfs/#what-is-the-dbfs-root)|[GCP](https://docs.gcp.databricks.com/dbfs/index.html#what-is-the-dbfs-root))„ÇÑ„Éû„Ç¶„É≥„Éà„Éù„Ç§„É≥„Éà([AWS](https://docs.databricks.com/dbfs/mounts.html)|[Azure](https://learn.microsoft.com/azure/databricks/dbfs/mounts)|[GCP](https://docs.gcp.databricks.com/dbfs/mounts.html))„ÇíÂà©Áî®„Åô„Çã„Åì„Å®„Åå„Åß„Åç„Åæ„Åô„ÄÇ
# MAGIC
# MAGIC DBFS„Çí‰ΩøÁî®„Åô„Çã„Åì„Å®„Åß„ÄÅ„É¢„Éá„É´Â≠¶Áøí„Å´‰ΩøÁî®„Åô„Çã`transformers`‰∫íÊèõ„ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„Éà„Çí‰ΩúÊàê„Åô„ÇãÈöõ„Å´„ÄÅ"„É≠„Éº„Ç´„É´"„Éë„Çπ„ÇíÂèÇÁÖß„Åô„Çã„Åì„Å®„Åå„Åß„Åç„Åæ„Åô„ÄÇ

# COMMAND ----------

# DBTITLE 1,„Éá„Éº„Çø„ÇíÂ≠¶ÁøíÁî®ÔºèÊ§úË®ºÁî®„Å´ÂàÜÂâ≤„Åó„ÄÅHuggigFace Dataset„Å´Â§âÊèõ
dataset_df = spark.read.table(table_name_full_path)
(train_df, test_df) = dataset_df.persist().randomSplit([0.8, 0.2], seed=47)

import datasets
train_dataset = datasets.Dataset.from_spark(train_df, cache_dir="/dbfs/cache/train")
test_dataset = datasets.Dataset.from_spark(test_df, cache_dir="/dbfs/cache/test")

# COMMAND ----------

# MAGIC %md
# MAGIC Â≠¶ÁøíÁî®„ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„Éà„Çí„Éà„Éº„ÇØ„É≥Âåñ„Åó„Å¶„Ç∑„É£„ÉÉ„Éï„É´„Åô„Çã„ÄÇ„Åæ„Åü„ÄÅÂ≠¶ÁøíÂá¶ÁêÜ„Åß„ÅØ`text`„Ç´„É©„É†„ÇíÂøÖË¶Å„Å®„Åó„Å™„ÅÑ„ÅÆ„Åß„ÄÅ„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åã„ÇâÂâäÈô§„Åó„Åæ„Åô„ÄÇÊú¨Ë≥™ÁöÑ„Å´„ÅØÂâäÈô§„Åó„Å™„Åè„Å®„ÇÇÂïèÈ°å„Å™„ÅÑ„ÅÆ„Åß„Åô„Åå„ÄÅ„Å™„Åú„ÅãÂàó„ÇíÂâäÈô§„Åó„Å™„ÅÑ„Å®„Éà„É¨„Éº„Éã„É≥„Ç∞‰∏≠„Å´Ë≠¶Âëä„ÅåÂá∫„Çã„Åü„ÇÅ„ÄÅ„ÅÇ„Åà„Å¶ÂâäÈô§„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ
# MAGIC „Åì„ÅÆ„Çπ„ÉÜ„ÉÉ„Éó„Åß„ÅØ„ÄÅ`datasets`„ÅØÂ§âÊèõ„Åï„Çå„Åü„Éá„Éº„Çø„Çª„ÉÉ„Éà„Çí„É≠„Éº„Ç´„É´„Éá„Ç£„Çπ„ÇØ„Å´„Ç≠„É£„ÉÉ„Ç∑„É•„Åó„ÄÅ„É¢„Éá„É´„ÅÆÂ≠¶ÁøíÊôÇ„Å´È´òÈÄü„Å´Ë™≠„ÅøËæº„ÇÅ„Çã„Çà„ÅÜ„Å´„Åô„Çã„ÄÇ

# COMMAND ----------

# DBTITLE 1,„ÉÜ„Ç≠„Çπ„ÉàÂàó„Åã„Çâ„Éà„Éº„ÇØ„É≥Âàó„Çí‰ΩúÊàêÔºàÊúÄÁµÇÁöÑ„Å´„ÉÜ„Ç≠„Çπ„ÉàÂàó„ÅØÂâäÈô§Ôºâ
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(base_model)

def tokenize_function(examples):
    return tokenizer(
        examples["text"], 
        max_length=max_length,
        padding='max_length', 
        truncation=True)

train_tokenized = train_dataset.map(tokenize_function, batched=True).remove_columns(["text"])
test_tokenized = test_dataset.map(tokenize_function, batched=True).remove_columns(["text"])
train_dataset = train_tokenized.shuffle(seed=47)
test_dataset = test_tokenized.shuffle(seed=47)

# COMMAND ----------

# DBTITLE 1,Datasets„Åã„Çâ1„É¨„Ç≥„Éº„ÉâÂèñ„ÇäÂá∫„Åó„ÄÅ‰∏≠Ë∫´„ÇíË¶ã„Å¶„Åø„Çã
train_dataset[0]

# COMMAND ----------

# MAGIC %md
# MAGIC ## 1-3. „É¢„Éá„É´„Éà„É¨„Éº„Éã„É≥„Ç∞

# COMMAND ----------

# MAGIC %md
# MAGIC „É≠„Ç∞„Å´Ë®òÈå≤„Åô„ÇãË©ï‰æ°ÊåáÊ®ô„ÇíÂÆöÁæ©„Åó„Åæ„Åô„ÄÇ‰ªäÂõû„ÅØAccuracy„ÇíË®òÈå≤„Åó„Åæ„Åô„ÄÇÊêçÂ§±ÔºàLossÔºâ„ÅØË®≠ÂÆö„Åõ„Åö„Å®„ÇÇËá™Âãï„Åß„É≠„Ç∞„Å´Ë®òÈå≤„Åï„Çå„Åæ„Åô„ÄÇ

# COMMAND ----------

import numpy as np
from datasets import load_metric

def compute_metrics(eval_pred):
   load_accuracy = load_metric("accuracy")

   logits, labels = eval_pred
   predictions = np.argmax(logits, axis=-1)
   accuracy = load_accuracy.compute(predictions=predictions, references=labels)["accuracy"]
   return {"accuracy": accuracy}

# COMMAND ----------

# MAGIC %md
# MAGIC Â≠¶ÁøíÁî®„ÅÆ„Éë„É©„É°„Éº„Çø„Éº„ÅØ„Åª„Åº„Éá„Éï„Ç©„É´„ÉàÂÄ§„Çí‰ΩøÁî®„Åó„Åæ„Åô„Åå„ÄÅEpochÊï∞Ôºà„Éá„Éï„Ç©„É´„Éà„ÅØ3Ôºâ„Å®„Éê„ÉÉ„ÉÅ„Çµ„Ç§„Ç∫Ôºà„Éá„Éï„Ç©„É´„Éà„ÅØ8Ôºâ„ÅÆ„Åø„Éá„Éï„Ç©„É´„Éà„Åã„ÇâÂ§âÊõ¥„Åó„Åæ„Åô„ÄÇ‰ªñ„Å´„ÇÇÂøÖË¶Å„Å´Âøú„Åò„Å¶Â≠¶ÁøíÁéá„Å™„Å©„ÅÆÂ§ö„Åè„ÅÆÂ≠¶Áøí„Éë„É©„É°„Éº„Çø„ÇíË®≠ÂÆö„Åß„Åç„Åæ„Åô„ÄÇË©≥Á¥∞„ÅØ
# MAGIC [transformers„Éâ„Ç≠„É•„É°„É≥„Éà](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments) „ÇíÂèÇÁÖß„Åè„Å†„Åï„ÅÑ„ÄÇ

# COMMAND ----------

from transformers import TrainingArguments

training_output_dir = f"{tutorial_path}/bert_trainer"
training_args = TrainingArguments(
  output_dir=training_output_dir, 
  logging_dir = f"{tutorial_path}/logs",    # TensorBoardÁî®„Å´„É≠„Ç∞„ÇíË®òÈå≤„Åô„Çã„Éá„Ç£„É¨„ÇØ„Éà„É™
  evaluation_strategy="epoch",
  num_train_epochs=2)

training_args.set_dataloader(train_batch_size=12, eval_batch_size=32)

print(f"Â≠¶ÁøíÊôÇ„ÅÆ„Éê„ÉÉ„ÉÅ„Çµ„Ç§„Ç∫„ÅØ„ÄÄ{training_args.per_device_train_batch_size}„ÄÅÊ§úË®ºÊôÇ„ÅÆ„Éê„ÉÉ„ÉÅ„Çµ„Ç§„Ç∫„ÅØ„ÄÄ{training_args.per_device_eval_batch_size}„ÄÄ„Åß„Åô„ÄÇ")


# COMMAND ----------

# MAGIC %md
# MAGIC „É©„Éô„É´„Éû„ÉÉ„Éî„É≥„Ç∞„Å®„ÇØ„É©„ÇπÊï∞„ÇíÊåáÂÆö„Åó„Å¶„ÄÅ„Éô„Éº„Çπ„É¢„Éá„É´„Åã„ÇâÂ≠¶Áøí„Åô„Çã„É¢„Éá„É´„Çí‰ΩúÊàê„Åó„Åæ„Åô„ÄÇ

# COMMAND ----------

from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained(
  base_model, 
  num_labels=len(category_list), 
  label2id=label2id, 
  id2label=id2label)

# COMMAND ----------

# MAGIC %md
# MAGIC [data collator](https://huggingface.co/docs/transformers/main_classes/data_collator)„Çí‰Ωø„ÅÜ„Åì„Å®„Åß„ÄÅË®ìÁ∑¥„Éá„Éº„Çø„Å®Ë©ï‰æ°„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅÆÂÖ•Âäõ„Çí„Éê„ÉÉ„ÉÅÂåñ„Åô„Çã„Åì„Å®„Åå„Åß„Åç„Çã„ÄÇ`DataCollatorWithPadding`„Çí„Éá„Éï„Ç©„É´„Éà„Åß‰ΩøÁî®„Åô„Çã„Å®„ÄÅ„ÉÜ„Ç≠„Çπ„ÉàÂàÜÈ°û„ÅÆ„Éô„Éº„Çπ„É©„Ç§„É≥ÊÄßËÉΩ„ÅåËâØ„Åè„Å™„Çã„ÄÇ

# COMMAND ----------

from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer)

# COMMAND ----------

# MAGIC %md
# MAGIC ‰∏äË®ò„Åß‰ΩúÊàê„Åó„Åü„É¢„Éá„É´„ÄÅÂºïÊï∞„ÄÅ„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÄÅÁÖßÂêàÂô®„ÄÅ„É°„Éà„É™„ÇØ„Çπ„ÇíÁî®„ÅÑ„Å¶„ÄÅ„Éà„É¨„Éº„Éä„Éº„Ç™„Éñ„Ç∏„Çß„ÇØ„Éà„ÇíÊßãÁØâ„Åó„Åæ„Åô„ÄÇ

# COMMAND ----------

from transformers import Trainer

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    compute_metrics=compute_metrics,
    data_collator=data_collator,
)

# COMMAND ----------

# MAGIC %md
# MAGIC „É¢„Éá„É´„Çí„Çµ„Éº„Éì„É≥„Ç∞„Ç®„É≥„Éâ„Éù„Ç§„É≥„Éà„Å®„Åó„Å¶„Éá„Éó„É≠„Ç§„Åô„Çã„Åü„ÇÅ„ÅÆMLFlowÁôªÈå≤Áî®„É©„ÉÉ„Éë„Éº„ÇØ„É©„Çπ„Åß„Åô„ÄÇ
# MAGIC Êú¨Êù•„ÅØMLFlow„ÅÆTransformer„Éï„É¨„Éº„Éê„Éº„Çí‰ΩøÁî®„Åô„Çã„Åì„Å®„Åß„Çà„ÇäÁ∞°Âçò„Å´ÁôªÈå≤„Åß„Åç„Çã„ÅÆ„Åß„Åô„Åå„ÄÅ„Åù„Çå„Å†„Å®„Çµ„Éº„Éì„É≥„Ç∞ÊôÇ„Å´GPU„Åå‰ΩøÁî®„Åï„Çå„Å™„ÅÑ„Ç±„Éº„Çπ„Åå„ÅÇ„Çã„Åü„ÇÅ„ÄÅGPU‰ΩøÁî®„Çí„Åô„Çã„Å´„ÅØ„Åì„ÅÆÂÆüË£Ö„ÅåÂøÖË¶Å„Åß„Åô„ÄÇ

# COMMAND ----------

import mlflow
import torch

class TextClassificationPipelineModel(mlflow.pyfunc.PythonModel):
  
  def __init__(self, model, tokenizer):
    device = 0 if torch.cuda.is_available() else -1
    self.pipeline = pipeline(
      "text-classification", 
      model=model, 
      tokenizer=tokenizer,
      batch_size=1,
      device=device)
    self.tokenizer = tokenizer
    
  def predict(self, context, model_input): 
    messages = model_input["text"].to_list()
    answers = self.pipeline(messages, max_length=max_length, padding='max_length', truncation=True)

    label_list = []
    score_list = []
    for answer in answers:
      label_list.append(answer['label'])
      score_list.append(str(answer['score']))

    return {"label": label_list, "score": score_list}

# COMMAND ----------

# MAGIC %md
# MAGIC „É¢„Éá„É´„Çí„Éà„É¨„Éº„Éã„É≥„Ç∞„Åó„ÄÅ„É°„Éà„É™„ÇØ„Çπ„Å®ÁµêÊûú„Çí MLflow „Å´Ë®òÈå≤„Åó„Åæ„Åô„ÄÇ
# MAGIC MLFlow„ÅÆTransformer„Éï„É¨„Éº„Éê„Éº„Çí‰ΩøÁî®„Åô„Çã„Åì„Å®„Åß„ÄÅÁ∞°Âçò„Å´Ë®òÈå≤„Åß„Åç„Åæ„Åô„ÄÇ

# COMMAND ----------

from transformers import pipeline

from mlflow.models.signature import ModelSignature
from mlflow.types import DataType, Schema, ColSpec

model_output_dir = f"{tutorial_path}/trained_model"
pipeline_output_dir = f"{tutorial_path}/trained_pipeline"

with mlflow.start_run() as run:
  
  # Â≠¶ÁøíÈñãÂßã„ÄÇÂ≠¶Áøí„ÅÆ„É°„Éà„É™„ÉÉ„ÇØ„ÅåËá™ÂãïÁöÑ„Å´MLFLow„Å´„É≠„ÇÆ„É≥„Ç∞„Åï„Çå„Çã
  trainer.train()

  # Â≠¶ÁøíÁµÇ‰∫ÜÂæå„Å´„É¢„Éá„É´„Çí‰øùÂ≠ò
  trainer.save_model(model_output_dir)
  
  # Â≠¶ÁøíÊ∏à„Åø„É¢„Éá„É´„ÇíË™≠„ÅøËæº„Çì„Åß„Éë„Ç§„Éó„É©„Ç§„É≥Âåñ„Åó„Å¶„ÄÅÊõ¥„Å´‰øùÂ≠ò„ÄÇ
  bert = AutoModelForSequenceClassification.from_pretrained(model_output_dir)

  # pipe = pipeline(
  #   "text-classification", 
  #   model=bert, 
  #   batch_size=1, 
  #   tokenizer=tokenizer,
  #   device=0)
  # pipe.save_pretrained(pipeline_output_dir)
  
  #######################################
  # CPU„ÅÆ„Åø„Åß„ÅÆ„Çµ„Éº„Éì„É≥„Ç∞„ÅßËâØ„Åë„Çå„Å∞„Åì„Å°„Çâ„Åß„ÇÇÂèØ
  #######################################
  # # MLFlow Tracking„Å´„Éë„Ç§„Éó„É©„Ç§„É≥„ÇíË®òÈå≤„Åô„Çã„ÄÇ
  # mlflow.transformers.log_model(
  #   transformers_model=pipe, 
  #   artifact_path=model_artifact_path+"_CPU", 
  #   input_example=["„Åì„Çå„ÅØ„Çµ„É≥„Éó„É´Ôºë„Åß„Åô„ÄÇ", "„Åì„Çå„ÅØ„Çµ„É≥„Éó„É´Ôºí„Åß„Åô„ÄÇ"],
  #   pip_requirements=["torch", "transformers", "accelerate", "sentencepiece", "datasets", "fugashi", "unidic-lite"],
  #   # registered_model_name=registered_model_name,
  #   model_config={ 
  #     "max_length": max_length, 
  #     "padding": "max_length", 
  #     "truncation": True 
  #   }
  # )

  # „Ç®„É≥„Éâ„Éù„Ç§„É≥„Éà„ÅÆÂÖ•Âäõ„Å®Âá∫Âäõ„ÅÆ„Çπ„Ç≠„Éº„Éû„ÇíÂÆöÁæ©
  input_schema = Schema([ColSpec(DataType.string, "text")])
  output_schema = Schema([ColSpec(DataType.string, "label"), ColSpec(DataType.double, "score")])
  signature = ModelSignature(inputs=input_schema, outputs=output_schema)

  # ÂÖ•Âäõ„Éá„Éº„Çø„ÅÆ„Çµ„É≥„Éó„É´„ÇíÁî®ÊÑè
  input_example = pd.DataFrame({"text": ["„Åì„Çå„ÅØ„Çµ„É≥„Éó„É´Ôºë„Åß„Åô„ÄÇ", "„Åì„Çå„ÅØ„Çµ„É≥„Éó„É´Ôºí„Åß„Åô„ÄÇ"]})
  
  # „É¢„Éá„É´„ÇíMLFlow Tracking„Å´Ë®òÈå≤
  mlflow.pyfunc.log_model(
      artifact_path=model_artifact_path,
      python_model=TextClassificationPipelineModel(bert, tokenizer),
      pip_requirements=["torch", "transformers", "accelerate", "sentencepiece", "datasets", "fugashi", "unidic-lite"],
      input_example=input_example,
      signature=signature
  )

print(f"„É¢„Éá„É´„ÅØMLFlowÂÆüÈ®ì„ÅÆRun(ID:{run.info.run_id})„Å´Ë®òÈå≤„Åï„Çå„Åæ„Åó„Åü„ÄÇ„Åì„ÅÆID„ÇíË®òÈå≤„Åó„Å¶„Åä„ÅÑ„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ")

# COMMAND ----------

# DBTITLE 1,TensorBoardÁî®„Å´„É≠„Ç∞„ÅÆ„Éá„Ç£„É¨„ÇØ„Éà„É™„Éº„ÇíÊåáÂÆö
# MAGIC %load_ext tensorboard
# MAGIC experiment_log_dir = f"{tutorial_path}/logs"

# COMMAND ----------

# DBTITLE 1,TensorBoardËµ∑Âãï
# MAGIC %tensorboard --logdir $experiment_log_dir

# COMMAND ----------

# MAGIC %md
# MAGIC ## 1-4. Â≠¶ÁøíÊ∏à„Åø„É¢„Éá„É´„ÅßÊé®Ë´ñ

# COMMAND ----------

# DBTITLE 1,„ÉÜ„Çπ„ÉàÁî®„ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„Éà
# „ÉÜ„Çπ„ÉàÁî®„ÅÆÊñáÁ´†„Éá„Éº„Çø
inputs = [
    """
„Ç™„Éº„Ç∑„É£„É≥„Ç∑„ÉÜ„Ç£„Å´ÊâÄÂ±û„Åô„Çã„Éç„Ç™„É©„É≥„Éâ‰ª£Ë°®FW„Ç¢„É¨„ÉÉ„ÇØ„Çπ„Éª„Çπ„Çø„Éº„Éû„É≥„Åå„ÄÅÂÖ®‰Ωì„Éà„É¨„Éº„Éã„É≥„Ç∞„Å´Âæ©Â∏∞„Åó„Åü„ÄÇ13Êó•„ÄÅ„ÇØ„É©„ÉñÂÖ¨Âºè„Çµ„Ç§„Éà„Åå‰ºù„Åà„Å¶„ÅÑ„Çã„ÄÇ
„Ç™„Éº„Ç∑„É£„É≥„Ç∑„ÉÜ„Ç£„ÇíÈõ¢„Çå„ÄÅ„Éç„Ç™„É©„É≥„Éâ‰ª£Ë°®„Åß„ÅÆ„Éü„Çπ„ÉÜ„Ç£„ÉÉ„ÇØ„Ç≥„É≥„ÉÅ„Éç„É≥„ÉÑ„Ç´„ÉÉ„Éó„Å´ÂèÇÊà¶„Åó„Å¶„ÅÑ„Åü„Çπ„Çø„Éº„Éû„É≥„ÅØ„ÄÅÂÖàÊúà18Êó•„Å´Ë°å„Çè„Çå„ÅüÁ¨¨2ÁØÄ„ÅÆ„Çµ„É≥„É©„Ç§„Éà‰ª£Ë°®Êà¶Ôºà‚ñ≥2Ôºç2Ôºâ„ÅßÂ∑¶Â§™„ÇÇ„ÇÇË£è„ÇíË≤†ÂÇ∑„Åó„Å¶„Åó„Åæ„ÅÑ„ÄÅÂâçÂçä„Ç¢„Éá„Ç£„Ç∑„Éß„Éä„É´„Çø„Ç§„É†„Å´ÈÄî‰∏≠‰∫§‰ª£„Çí‰ΩôÂÑÄ„Å™„Åè„Åï„Çå„ÄÅ„ÉÅ„Éº„É†„ÇíÈõ¢ËÑ±„Åó„Å¶„Ç™„Éº„Ç∑„É£„É≥„Ç∑„ÉÜ„Ç£„Å´Âæ©Â∏∞„Åó„Å¶„ÅÑ„Åü„ÄÇ
    """,
    """
M3„Ç∑„É™„Éº„Ç∫„ÅØ„ÄÅ8„Ç≥„Ç¢„ÅÆCPU„Å®10„Ç≥„Ç¢„ÅÆGPU„ÇíÂÇô„Åà„ÄÅÊúÄÂ§ß„Åß24GB„ÅÆÁµ±Âêà„É°„É¢„É™„Çí„Çµ„Éù„Éº„Éà„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇM3 Pro„É¢„Éá„É´„Åß„ÅØ„ÄÅCPU„ÇíÊúÄÂ§ß12„Ç≥„Ç¢„ÄÅGPU„ÇíÊúÄÂ§ß18„Ç≥„Ç¢„Åæ„ÅßÈÅ∏ÊäûÂèØËÉΩ„Åß„ÄÅÁµ±Âêà„É°„É¢„É™„ÅØÊúÄÂ§ß„Åß36GB„Åß„Åô„ÄÇ‰∏ÄÊñπ„ÄÅM3 Max„Åß„ÅØCPU„ÇíÊúÄÂ§ß„Åß16„Ç≥„Ç¢„ÄÅGPU„ÇíÊúÄÂ§ß„Åß40„Ç≥„Ç¢„ÄÅÁµ±Âêà„É°„É¢„É™„ÇíÊúÄÂ§ß„Åß128GB„Åæ„ÅßÈÅ∏Êäû„Åß„Åç„Åæ„Åô„ÄÇ
„Åì„Çå„ÇâÂÖ®„É¢„Éá„É´„Å´ÂÖ±ÈÄö„Åó„Å¶„ÄÅAV1„Ç≥„Éº„Éá„ÉÉ„ÇØ„ÅÆ„Éá„Ç≥„Éº„Éâ„Çí„Çµ„Éù„Éº„Éà„Åô„ÇãÂÜÖËîµ„É°„Éá„Ç£„Ç¢„Ç®„É≥„Ç∏„É≥„ÇíÊê≠Ëºâ„Åó„Å¶„Åä„Çä„ÄÅHEVC„ÄÅH.264„ÄÅProRes„Å™„Å©„ÅÆÊßò„ÄÖ„Å™„Éï„Ç©„Éº„Éû„ÉÉ„Éà„ÅÆÂÜçÁîü„ÅåÂèØËÉΩ„Åß„Åô„ÄÇ„Åï„Çâ„Å´„ÄÅDolby Vision„ÄÅHDR 10„ÄÅHLG„Å®„ÅÑ„Å£„ÅüÈ´ò„ÉÄ„Ç§„Éä„Éü„ÉÉ„ÇØ„É¨„É≥„Ç∏„Éï„Ç©„Éº„Éû„ÉÉ„Éà„Å´„ÇÇÂØæÂøú„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ
    """,
    """
„ÄÄ„Åó„Åã„Åó„ÄÅÊÅãÊÑõ„Å´„Åä„ÅÑ„Å¶ÂèóÂãïÁöÑ„Å™ÂßøÂã¢„Åã„ÇâÂßã„ÇÅ„Çã„Åì„Å®„Åå„ÄÅÊ•Ω„Å†„Å®ÊÑü„Åò„Å¶„Åó„Åæ„ÅÜ„Å®„ÄÅÈñ¢‰øÇ„ÅåÂßã„Åæ„Å£„Å¶„Åã„ÇâÂπ≥Á≠â„Å™‰ø°È†ºÈñ¢‰øÇ„ÇíÊßãÁØâ„Åô„Çã„ÅÆ„ÅåÈõ£„Åó„Åè„Å™„Çä„Åå„Å°„Åß„Åô„ÄÇÁî∑ÊÄß„Åå„É™„Éº„Éâ„Åó„ÄÅÂ•≥ÊÄß„Åå„Åù„Çå„Å´Âæì„ÅÜ„Å®„ÅÑ„ÅÜÈñ¢‰øÇ„ÅåÂÆöÁùÄ„Åô„Çã„Å®„ÄÅÂ•≥ÊÄß„ÅØÂæê„ÄÖ„Å´„ÄåÂ´å„Çè„Çå„Åü„Åè„Å™„ÅÑ„Äç„Å®„ÅÑ„ÅÜÊÄù„ÅÑ„Åã„ÇâËá™„Çâ„ÅÆÊÑèË¶ã„ÇíËø∞„Åπ„Å´„Åè„Åè„Å™„Çä„Åæ„Åô„ÄÇ„Åù„Åó„Å¶„ÄÅÂ†¥Âêà„Å´„Çà„Å£„Å¶„ÅØ„ÄÅÁî∑ÊÄß„ÅåÊîØÈÖçÁöÑ„Å™ÊÖãÂ∫¶„Çí„Å®„Çã„Çà„ÅÜ„Å´„Å™„Çä„ÄÅ„Åù„ÅÆ„Çà„ÅÜ„Å™Èñ¢‰øÇ„Åã„ÇâÊäú„ÅëÂá∫„Åô„ÅÆ„ÅåÂõ∞Èõ£„Å´„Å™„Çã„Åì„Å®„ÇÇ„ÅÇ„Çã„ÅÆ„Åß„Åô„ÄÇ
    """,
    """
21‰∏ñÁ¥Ä„Éî„ÇØ„ÉÅ„É£„Éº„Ç∫„Åå„ÄÅ‰∫∫Ê∞óSF„Ç¢„ÇØ„Ç∑„Éß„É≥„Äé„Éè„É≥„Çø„Éº„Ç∫„Äè„Ç∑„É™„Éº„Ç∫„ÅÆÊñ∞‰ΩúÊò†Áîª„Äé„ÉÄ„Éº„ÇØ„Éï„Ç£„Éº„É´„ÉâÔºàÂéüÈ°åÔºâ / Darkfield„Äè„ÅÆË£Ω‰Ωú„ÇíÈÄ≤„ÇÅ„Å¶„ÅÑ„Çã„Å®„ÄÅThe Global Film Gazette „Å™„Å©„ÅåÂ†±„Åò„Åü„ÄÇ
„ÄÄÁõ£Áù£„ÅØÂâç‰Ωú„Äé„Éè„É≥„Çø„Éº„Ç∫Ôºö„Ç∂„Éª„ÇØ„Ç®„Çπ„Éà„ÄèÔºà2022Ôºâ„Å®Âêå„Åò„Åè„Ç∏„Éß„É≥„Éª„Éâ„Éº„É¥„Ç°„Éº„ÅåÂãô„ÇÅ„Çã„Åå„ÄÅÂêå„Çµ„Ç§„Éà„Å´„Çà„Çã„Å®„ÄÅÊñ∞‰Ωú„ÅØ„Äé„Ç∂„Éª„ÇØ„Ç®„Çπ„Éà„Äè„ÅÆÁ∂öÁ∑®„Å´„ÅØ„Å™„Çâ„Å™„ÅÑ„Å®„ÅÆ„Åì„Å®„ÄÇË©≥Á¥∞„ÅØ‰∏çÊòé„Å†„Åå„ÄÅÊú™Êù•„ÇíËàûÂè∞„Å´„ÄÅ„Äé„Ç∂„Éª„ÇØ„Ç®„Çπ„Éà„Äè„Å®Âêå„Åò„ÅèÂ•≥ÊÄß„Åå‰∏ª‰∫∫ÂÖ¨„Å´„Å™„Çã„ÄÇ
    """
]

# COMMAND ----------

# DBTITLE 1,Â≠¶ÁøíÊ∏à„Åø„É¢„Éá„É´„Çí‰Ωø„Å£„Å¶„Éë„Ç§„Éó„É©„Ç§„É≥„Çí‰ΩúÊàê„ÅóÊé®Ë´ñÂÆüË°å
trained_pipe = pipeline(
    "text-classification", 
    model=AutoModelForSequenceClassification.from_pretrained(model_output_dir), 
    batch_size=4, 
    top_k=3,
    tokenizer=tokenizer,
    device=0)
res = trained_pipe(inputs)
res

# COMMAND ----------

# MAGIC %md
# MAGIC ## „ÅäÁñ≤„ÇåÊßò„Åß„Åó„ÅüÔºÅ

# COMMAND ----------

# MAGIC %md
# MAGIC ---

# COMMAND ----------

# MAGIC %md
# MAGIC ## „Åä„Åæ„Åë: TorchDistributer„ÇíÁî®„ÅÑ„ÅüÂàÜÊï£Â≠¶Áøí
# MAGIC TorchDistributor „ÅØ PySpark „ÅÆ„Ç™„Éº„Éó„É≥„ÇΩ„Éº„Çπ „É¢„Ç∏„É•„Éº„É´„Åß„ÅÇ„Çä„ÄÅ„É¶„Éº„Ç∂„Éº„Åå Spark „ÇØ„É©„Çπ„Çø„Éº„Åß PyTorch „Çí‰ΩøÁî®„Åó„Å¶ÂàÜÊï£„Éà„É¨„Éº„Éã„É≥„Ç∞„ÇíË°å„ÅÜ„ÅÆ„Å´ÂΩπÁ´ã„Å§„Åü„ÇÅ„ÄÅPyTorch „Éà„É¨„Éº„Éã„É≥„Ç∞ „Ç∏„Éß„Éñ„Çí Spark „Ç∏„Éß„Éñ„Å®„Åó„Å¶Ëµ∑Âãï„Åß„Åç„Åæ„Åô„ÄÇ ÂÜÖÈÉ®ÁöÑ„Å´„ÅØ„ÄÅ„ÉØ„Éº„Ç´„ÉºÈñì„ÅÆÁí∞Â¢É„Å®ÈÄö‰ø°„ÉÅ„É£„Éç„É´„ÇíÂàùÊúüÂåñ„Åó„ÄÅCLI„Ç≥„Éû„É≥„Éâ torch.distributed.run „ÇíÂà©Áî®„Åó„Å¶„ÉØ„Éº„Ç´„Éº„Éé„Éº„ÉâÈñì„ÅßÂàÜÊï£„Éà„É¨„Éº„Éã„É≥„Ç∞„ÇíÂÆüË°å„Åó„Åæ„Åô„ÄÇ
# MAGIC
# MAGIC ÂèÇËÄÉÔºöhttps://docs.databricks.com/ja/machine-learning/train-model/distributed-training/spark-pytorch-distributor.html

# COMMAND ----------

import torch
 
NUM_WORKERS = 4
 
def get_gpus_per_worker(_):
  import torch
  return torch.cuda.device_count()
 
NUM_GPUS_PER_WORKER = sc.parallelize(range(4), 4).map(get_gpus_per_worker).collect()[0]
USE_GPU = NUM_GPUS_PER_WORKER > 0

# COMMAND ----------

from transformers import AutoModelForSequenceClassification

def train_model():
    from transformers import TrainingArguments, Trainer
 
    training_args = TrainingArguments(
      output_dir=model_output_dir, 
      evaluation_strategy="epoch",
      save_strategy="epoch",
      report_to=[], # REMOVE MLFLOW INTEGRATION FOR NOW
      push_to_hub=False,  # DO NOT PUSH TO MODEL HUB FOR NOW,
      load_best_model_at_end=True, # RECOMMENDED
      metric_for_best_model="eval_loss", # RECOMMENDED
      num_train_epochs=5)
 
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=test_dataset,
        compute_metrics=compute_metrics,
        data_collator=data_collator,
    )
    
    trainer.train()
    return trainer.state.best_model_checkpoint
 
# It is recommended to create a separate local trainer from pretrained model instead of using the trainer used in distributed training
def test_model(ckpt_path):
    model = AutoModelForSequenceClassification.from_pretrained(ckpt_path)
    local_trainer = Trainer(
        model=model,
        eval_dataset=test_dataset,
        tokenizer=tokenizer,
        data_collator=data_collator,
        compute_metrics=compute_metrics
    )
    return local_trainer.evaluate()

# COMMAND ----------

# MAGIC %md
# MAGIC ### „Ç∑„É≥„Ç∞„É´„Éé„Éº„Éâ„Åß„Éû„É´„ÉÅGPU

# COMMAND ----------

from pyspark.ml.torch.distributor import TorchDistributor

NUM_PROCESSES = torch.cuda.device_count()
print(f"We're using {NUM_PROCESSES} GPUs")
single_node_multi_gpu_ckpt_path = TorchDistributor(
  num_processes=NUM_PROCESSES, 
  local_mode=True, 
  use_gpu=USE_GPU).run(train_model)

# COMMAND ----------

test_model(single_node_multi_gpu_ckpt_path)

# COMMAND ----------

# MAGIC %md
# MAGIC ### „Éû„É´„ÉÅ„Éé„Éº„Éâ„Åß„Ç∑„É≥„Ç∞„É´/„Éû„É´„ÉÅGPU

# COMMAND ----------

from pyspark.ml.torch.distributor import TorchDistributor
 
NUM_PROCESSES = NUM_GPUS_PER_WORKER * NUM_WORKERS
print(f"We're using {NUM_PROCESSES} GPUs")
multi_node_ckpt_path = TorchDistributor(
  num_processes=NUM_PROCESSES, 
  local_mode=False, 
  use_gpu=USE_GPU).run(train_model)

# COMMAND ----------

test_model(multi_node_ckpt_path)

# COMMAND ----------

# MAGIC %md
# MAGIC ## ‰ª•‰∏ä„Åß„Åô„ÄÇ
