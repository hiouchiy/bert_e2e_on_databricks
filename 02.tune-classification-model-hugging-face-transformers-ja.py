# Databricks notebook source
# MAGIC %md
# MAGIC # Hugging Face ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã§ãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡ãƒ¢ãƒ‡ãƒ«ã‚’ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹
# MAGIC ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¯ã€"cl-tohoku/bert-base-japanese-whole-word-masking"ã‚’ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã¨ã—ãŸè¨˜äº‹åˆ†é¡å™¨ã‚’ã‚·ãƒ³ã‚°ãƒ«GPUãƒã‚·ãƒ³ã§å­¦ç¿’ã—ã¾ã™ã€‚
# MAGIC Transformers](https://huggingface.co/docs/transformers/index)ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ã„ã¾ã™ã€‚
# MAGIC
# MAGIC ã¾ãšå°ã•ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã€Spark DataFrameã«å¤‰æ›ã—ã€Unity Catalogã¸Delta Tableã¨ã—ã¦ä¿å­˜ã—ã¾ã™ã€‚ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã¾ã§ã®å‰å‡¦ç†ã¯Sparkä¸Šã§è¡Œã‚ã‚Œã¾ã™ã€‚DBFSã¯ã€ãƒ‰ãƒ©ã‚¤ãƒä¸Šã®ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ç›´æ¥ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹ãŸã‚ã®ä¾¿å®œã¨ã—ã¦ä½¿ç”¨ã•ã‚Œã¦ã¾ã™ãŒã€DBFSã‚’ä½¿ç”¨ã—ãªã„ã‚ˆã†ã«å¤‰æ›´ã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚
# MAGIC
# MAGIC è¨˜äº‹ã®ãƒ†ã‚­ã‚¹ãƒˆãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ã¯ã€ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã¨ã®ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã®ä¸€è²«æ€§ã‚’æŒãŸã›ã‚‹ãŸã‚ã«ã€ãƒ¢ãƒ‡ãƒ«ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®`transformers`ã§è¡Œã‚ã‚Œã¾ã™ã€‚ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¯ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹ãŸã‚ã« `transformers` ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã® [Trainer](https://huggingface.co/docs/transformers/main_classes/trainer) ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¯ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã¨å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’Transformersã®`ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³`ã«ãƒ©ãƒƒãƒ—ã—ã€ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’MLflowãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦ãƒ­ã‚°ã«è¨˜éŒ²ã—ã¾ã™ã€‚
# MAGIC ã“ã‚Œã«ã‚ˆã‚Šã€ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ Spark DataFrame ã®æ–‡å­—åˆ—ã‚«ãƒ©ãƒ ã® UDF ã¨ã—ã¦ç›´æ¥é©ç”¨ã™ã‚‹ã“ã¨ãŒç°¡å˜ã«ãªã‚Šã¾ã™ã€‚
# MAGIC
# MAGIC ## ã‚¯ãƒ©ã‚¹ã‚¿ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
# MAGIC ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã¯ã€AWS ã® `g4dn.xlarge` ã‚„ Azure ã® `Standard_NC4as_T4_v3` ã®ã‚ˆã†ãªã‚·ãƒ³ã‚°ãƒ« GPU ã‚¯ãƒ©ã‚¹ã‚¿ã‚’æ¨å¥¨ã—ã¾ã™ã€‚ã‚·ãƒ³ã‚°ãƒ«ãƒã‚·ãƒ³ã‚¯ãƒ©ã‚¹ã‚¿ã®ä½œæˆ](https://docs.databricks.com/clusters/configure.html) ã¯ã€ãƒ‘ãƒ¼ã‚½ãƒŠãƒ«ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ãƒˆãƒãƒªã‚·ãƒ¼ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã€ã‚¯ãƒ©ã‚¹ã‚¿ä½œæˆæ™‚ã« "Single Node" ã‚’é¸æŠã™ã‚‹ã“ã¨ã§å¯èƒ½ã§ã™ã€‚ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¯Databricks Runtime ML GPUãƒãƒ¼ã‚¸ãƒ§ãƒ³14.3 LTSã§å‹•ä½œç¢ºèªã—ã¦ãŠã‚Šã¾ã™ã€‚
# MAGIC
# MAGIC Databricks Runtime ML ã«ã¯ `transformers` ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã™ã€‚ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã¯ã€[ğŸ¤—&nbsp;Datasets](https://huggingface.co/docs/datasets/index) ã¨ [ğŸ¤—&nbsp;Evaluate](https://huggingface.co/docs/evakyate/index)ã‚‚å¿…è¦ã§ã€ã“ã‚Œã‚‰ã¯ `%pip` ã‚’ä½¿ã£ã¦ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã§ãã¾ã™ã€‚

# COMMAND ----------

# MAGIC %md
# MAGIC # 0. ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã¨ã‚¤ãƒ³ãƒãƒ¼ãƒˆ

# COMMAND ----------

# MAGIC %pip install datasets evaluate fugashi unidic-lite accelerate
# MAGIC dbutils.library.restartPython()

# COMMAND ----------

# DBTITLE 1,ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã®è¨­å®š
################################################
# ãƒ¦ãƒ¼ã‚¶ãƒ¼æ¯ã«å¤‰æ›´ãŒä¸è¦ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼
################################################

# HuggingFace ãƒ¢ãƒ‡ãƒ«å
base_model = "tohoku-nlp/bert-base-japanese-v3"

# BERTã«æŠ•å…¥ã™ã‚‹æ–‡ç« ã®æœ€å¤§é•·
max_length = 128

# ãƒ‡ãƒ¼ã‚¿ãªã©ã‚’æ ¼ç´ã™ã‚‹ãƒ‘ã‚¹
# ã“ã®ãƒ‡ãƒ¢ã§ã¯ãƒ‰ãƒ©ã‚¤ãƒãƒ¼ãƒãƒ¼ãƒ‰ã®ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‡ã‚£ã‚¹ã‚¯ã«æ ¼ç´ã—ã¾ã™ãŒã€ä»¥ä¸‹ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«å‰‡ã‚ŠDBFSãªã©ä»–ã®é©åˆ‡ãªå ´æ‰€ã«æ ¼ç´ã‚‚å¯èƒ½ã§ã™
# https://docs.databricks.com/ja/files/write-data.html
tutorial_path = "/databricks/driver" # 
import os
os.environ['TUTORIAL_PATH']=tutorial_path # å¾Œã»ã©Shellã‚³ãƒãƒ³ãƒ‰ã‹ã‚‰ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹ãŸã‚ç’°å¢ƒå¤‰æ•°ã«ã‚»ãƒƒãƒˆ

# MLFlow Trackingã«è¨˜éŒ²ã•ã‚Œã¦ã„ã‚‹ãƒ¢ãƒ‡ãƒ«åï¼ˆã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆåï¼‰
model_artifact_path = "bert_model_ja"


################################################
# ãƒ¦ãƒ¼ã‚¶ãƒ¼æ¯ã«å¤‰æ›´ãŒå¿…è¦ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼
################################################

# Unity Catalog ã‚«ã‚¿ãƒ­ã‚°å
catalog_name = "YOUR_CATALOG_NAME"

# Unity Catalog ã‚¹ã‚­ãƒ¼ãƒå
schema_name = "YOUR_SCHEMA_NAME"

# Unity Catalog ãƒ†ãƒ¼ãƒ–ãƒ«å
table_name = "YOUR_TABLE_NAME"

# ãƒ‡ãƒ¼ã‚¿ã‚’æ ¼ç´ã™ã‚‹ãŸã‚ã®ãƒ†ãƒ¼ãƒ–ãƒ«åï¼ˆcatalog_name.schema_name.table_nameï¼‰
table_name_full_path = f"{catalog_name}.{schema_name}.{table_name}"

# COMMAND ----------

# MAGIC %md
# MAGIC # 1. BERTã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°

# COMMAND ----------

# MAGIC %md
# MAGIC ## 1-1. ãƒ‡ãƒ¼ã‚¿ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã¨ãƒ­ãƒ¼ãƒ‰
# MAGIC ã¾ãšãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚ãã®å¾Œã€Spark DataFrameã¨ã—ã¦ãƒ­ãƒ¼ãƒ‰ã—ã€Delta Tableã¨ã—ã¦Unity Catalogã¸ä¿å­˜ã—ã¾ã™ã€‚
# MAGIC
# MAGIC ä»Šå›ç”¨ã„ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ[livedoor ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‘ã‚¹](https://www.rondhuit.com/download/ldcc-20140209.tar.gz)ã¯ã€[Rondhuitã®ã‚µã‚¤ãƒˆ](https://www.rondhuit.com/download.html)ã‹ã‚‰å…¥æ‰‹ã§ãã¾ã™ã€‚

# COMMAND ----------

# DBTITLE 1,ãƒ‡ãƒ¼ã‚¿ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼†è§£å‡ï¼ˆã‚¢ã‚¿ãƒƒãƒã•ã‚ŒãŸVMã®ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‡ã‚£ã‚¹ã‚¯ã«ä¸€æ™‚ä¿å­˜ï¼‰
# MAGIC %sh
# MAGIC cd $TUTORIAL_PATH
# MAGIC
# MAGIC # ãƒ‡ãƒ¼ã‚¿ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
# MAGIC wget https://www.rondhuit.com/download/ldcc-20140209.tar.gz
# MAGIC
# MAGIC # ãƒ•ã‚¡ã‚¤ãƒ«ã®è§£å‡
# MAGIC tar -zxf ldcc-20140209.tar.gz 

# COMMAND ----------

# DBTITLE 1,./textä»¥ä¸‹ã«ãã‚Œãã‚Œã®ãƒ¡ãƒ‡ã‚£ã‚¢ã«å¯¾å¿œã™ã‚‹ãƒ•ã‚©ãƒ«ãƒ€ãŒã‚ã‚‹ã®ã‚’ç¢ºèª
# MAGIC %sh
# MAGIC ls $TUTORIAL_PATH/text

# COMMAND ----------

# DBTITLE 1,é©å½“ãªãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‚’ç¢ºèªï¼ˆæœ€åˆã®3è¡Œã¯ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã§ã€4è¡Œç›®ã‹ã‚‰æœ¬æ–‡ï¼‰
# MAGIC %sh
# MAGIC cat $TUTORIAL_PATH/text/it-life-hack/it-life-hack-6342280.txt

# COMMAND ----------

# DBTITLE 1,ãƒ•ã‚¡ã‚¤ãƒ«åã¨ã‚¯ãƒ©ã‚¹ã®ä¸€è¦§ã‚’Pandas Dataframeã¨ã—ã¦ä½œæˆ
import glob
import pandas as pd

# ã‚«ãƒ†ã‚´ãƒªãƒ¼ã®ãƒªã‚¹ãƒˆ
category_list = [
    'dokujo-tsushin',
    'it-life-hack',
    'kaden-channel',
    'livedoor-homme',
    'movie-enter',
    'peachy',
    'smax',
    'sports-watch',
    'topic-news'
]

# å„ãƒ‡ãƒ¼ã‚¿ã®å½¢å¼ã‚’æ•´ãˆã‚‹
columns = ['label', 'label_name', 'file_path']
dataset_label_text = pd.DataFrame(columns=columns)
id2label = {} 
label2id = {}
for label, category in enumerate(category_list):
  
  file_names_list = sorted(glob.glob(f'{tutorial_path}/text/{category}/{category}*'))#å¯¾è±¡ãƒ¡ãƒ‡ã‚£ã‚¢ã®è¨˜äº‹ãŒä¿å­˜ã•ã‚Œã¦ã„ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã®listã‚’å–å¾—ã—ã¾ã™ã€‚
  print(f"{category}ã®è¨˜äº‹ã‚’å‡¦ç†ã—ã¦ã„ã¾ã™ã€‚ã€€{category}ã«å¯¾å¿œã™ã‚‹ç•ªå·ã¯{label}ã§ã€ãƒ‡ãƒ¼ã‚¿å€‹æ•°ã¯{len(file_names_list)}ã§ã™ã€‚")

  id2label[label] = category
  label2id[category] = label
  
  for file in file_names_list:
      list = [[label, category, file]]
      df_append = pd.DataFrame(data=list, columns=columns)
      dataset_label_text = pd.concat([dataset_label_text, df_append], ignore_index=True, axis=0)

dataset_label_text.head()

# COMMAND ----------

# DBTITLE 1,ãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹é–¢æ•°ï¼ˆPandas UDFï¼‰
from pyspark.sql.functions import pandas_udf
from pyspark.sql.types import StringType

@pandas_udf(StringType())
def read_text(paths: pd.Series) -> pd.Series:

  all_text = []
  for index, file in paths.items():             #å–å¾—ã—ãŸlistã«å¾“ã£ã¦å®Ÿéš›ã®Fileã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¾ã™ã€‚
      lines = open(file).read().splitlines()
      text = '\n'.join(lines[3:])               # ãƒ•ã‚¡ã‚¤ãƒ«ã®4è¡Œç›®ã‹ã‚‰ã‚’æŠœãå‡ºã™ã€‚
      all_text.append(text)

  return pd.Series(all_text)

# COMMAND ----------

# DBTITLE 1,Pandas Dataframeã‹ã‚‰Spark Dataframeã«å¤‰æ›ã—ã€ãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‚’æ–°è¦åˆ—ã¨ã—ã¦è¿½åŠ 
from pyspark.sql.functions import col

dataset_df = spark.createDataFrame(dataset_label_text)
dataset_df = dataset_df.withColumn('text', read_text(col('file_path')))
display(dataset_df.head(5))

# COMMAND ----------

# DBTITLE 1,æ¬¡ã®ã‚»ãƒ«ï¼ˆSQLï¼‰ã«æ¸¡ã™ãŸã‚ã«å¿…è¦ãªå¤‰æ•°ã‚’ã‚·ã‚¹ãƒ†ãƒ å¤‰æ•°ã¨ã—ã¦ã‚»ãƒƒãƒˆ
spark.conf.set("my.catalogName", catalog_name)
spark.conf.set("my.schemaName", schema_name)

# COMMAND ----------

# DBTITLE 1,Unity Catalogã«ã‚«ã‚¿ãƒ­ã‚°ã¨ã‚¹ã‚­ãƒ¼ãƒã‚’ç„¡ã‘ã‚Œã°ä½œã‚‹
# MAGIC %sql
# MAGIC CREATE CATALOG IF NOT EXISTS ${my.catalogName};
# MAGIC USE CATALOG ${my.catalogName};
# MAGIC CREATE SCHEMA IF NOT EXISTS ${my.catalogName}.${my.schemaName};
# MAGIC USE SCHEMA ${my.schemaName};

# COMMAND ----------

# DBTITLE 1,Spark Dataframeã‚’Delta Tableã¨ã—ã¦Unity Catalogã¸ä¿å­˜
dataset_df.write.mode("overwrite").saveAsTable(table_name_full_path)

# COMMAND ----------

# MAGIC %md
# MAGIC ## 1-2. ãƒ‡ãƒ¼ã‚¿åŠ å·¥
# MAGIC
# MAGIC Spark Dataframeã‹ã‚‰Hugging Face Datasetsã¸å¤‰æ›ã‚’ã—ã€textåˆ—ï¼ˆæ–‡ç« ãƒ‡ãƒ¼ã‚¿ï¼‰ã®å†…å®¹ã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã—ã¦ã€æ–°ãŸãªåˆ—ã¨ã—ã¦è¿½åŠ ã—ã¾ã™ã€‚
# MAGIC
# MAGIC Hugging Face ã® `datasets` ã¯ `datasets.Dataset.from_spark` ã‚’ä½¿ã£ã¦ Spark DataFrames ã‹ã‚‰ã®ãƒ­ãƒ¼ãƒ‰ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã™ã€‚[from_spark()](https://huggingface.co/docs/datasets/use_with_spark) ãƒ¡ã‚½ãƒƒãƒ‰ã®è©³ç´°ã«ã¤ã„ã¦ã¯ Hugging Face ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚ã¾ãŸã€Databricksã®[ã“ã¡ã‚‰ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](https://docs.databricks.com/ja/machine-learning/train-model/huggingface/load-data.html)ã‚‚å‚è€ƒã«ã—ã¦ãã ã•ã„ã€‚
# MAGIC
# MAGIC Dataset.from_sparkã¯ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã—ã¾ã™ã€‚ã“ã®ä¾‹ã§ã¯ã€ãƒ¢ãƒ‡ãƒ«ã¯ãƒ‰ãƒ©ã‚¤ãƒä¸Šã§å­¦ç¿’ã•ã‚Œã€ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã¯Sparkã‚’ä½¿ç”¨ã—ã¦ä¸¦åˆ—åŒ–ã•ã‚Œã‚‹ãŸã‚ã€`cache_dir`ã¯ãƒ‰ãƒ©ã‚¤ãƒã¨ã™ã¹ã¦ã®ãƒ¯ãƒ¼ã‚«ãƒ¼ã‹ã‚‰ã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½ã§ãªã‘ã‚Œã°ãªã‚Šã¾ã›ã‚“ã€‚Databricks File System (DBFS)ã®ãƒ«ãƒ¼ãƒˆ([AWS](https://docs.databricks.com/dbfs/index.html#what-is-the-dbfs-root)|[Azure](https://learn.microsoft.com/azure/databricks/dbfs/#what-is-the-dbfs-root)|[GCP](https://docs.gcp.databricks.com/dbfs/index.html#what-is-the-dbfs-root))ã‚„ãƒã‚¦ãƒ³ãƒˆãƒã‚¤ãƒ³ãƒˆ([AWS](https://docs.databricks.com/dbfs/mounts.html)|[Azure](https://learn.microsoft.com/azure/databricks/dbfs/mounts)|[GCP](https://docs.gcp.databricks.com/dbfs/mounts.html))ã‚’åˆ©ç”¨ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚
# MAGIC
# MAGIC DBFSã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã§ã€ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã«ä½¿ç”¨ã™ã‚‹`transformers`äº’æ›ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆã™ã‚‹éš›ã«ã€"ãƒ­ãƒ¼ã‚«ãƒ«"ãƒ‘ã‚¹ã‚’å‚ç…§ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

# COMMAND ----------

# DBTITLE 1,ãƒ‡ãƒ¼ã‚¿ã‚’å­¦ç¿’ç”¨ï¼æ¤œè¨¼ç”¨ã«åˆ†å‰²ã—ã€HuggigFace Datasetã«å¤‰æ›
dataset_df = spark.read.table(table_name_full_path)
(train_df, test_df) = dataset_df.persist().randomSplit([0.8, 0.2], seed=47)

import datasets
train_dataset = datasets.Dataset.from_spark(train_df, cache_dir="/dbfs/cache/train")
test_dataset = datasets.Dataset.from_spark(test_df, cache_dir="/dbfs/cache/test")

# COMMAND ----------

# MAGIC %md
# MAGIC å­¦ç¿’ç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã—ã¦ã‚·ãƒ£ãƒƒãƒ•ãƒ«ã™ã‚‹ã€‚ã¾ãŸã€å­¦ç¿’å‡¦ç†ã§ã¯`text`ã‚«ãƒ©ãƒ ã‚’å¿…è¦ã¨ã—ãªã„ã®ã§ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã‚‰å‰Šé™¤ã—ã¾ã™ã€‚æœ¬è³ªçš„ã«ã¯å‰Šé™¤ã—ãªãã¨ã‚‚å•é¡Œãªã„ã®ã§ã™ãŒã€ãªãœã‹åˆ—ã‚’å‰Šé™¤ã—ãªã„ã¨ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ä¸­ã«è­¦å‘ŠãŒå‡ºã‚‹ãŸã‚ã€ã‚ãˆã¦å‰Šé™¤ã—ã¦ã„ã¾ã™ã€‚
# MAGIC ã“ã®ã‚¹ãƒ†ãƒƒãƒ—ã§ã¯ã€`datasets`ã¯å¤‰æ›ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‡ã‚£ã‚¹ã‚¯ã«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã—ã€ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’æ™‚ã«é«˜é€Ÿã«èª­ã¿è¾¼ã‚ã‚‹ã‚ˆã†ã«ã™ã‚‹ã€‚

# COMMAND ----------

# DBTITLE 1,ãƒ†ã‚­ã‚¹ãƒˆåˆ—ã‹ã‚‰ãƒˆãƒ¼ã‚¯ãƒ³åˆ—ã‚’ä½œæˆï¼ˆæœ€çµ‚çš„ã«ãƒ†ã‚­ã‚¹ãƒˆåˆ—ã¯å‰Šé™¤ï¼‰
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(base_model)

def tokenize_function(examples):
    return tokenizer(
        examples["text"], 
        max_length=max_length,
        padding='max_length', 
        truncation=True)

train_tokenized = train_dataset.map(tokenize_function, batched=True).remove_columns(["text"])
test_tokenized = test_dataset.map(tokenize_function, batched=True).remove_columns(["text"])
train_dataset = train_tokenized.shuffle(seed=47)
test_dataset = test_tokenized.shuffle(seed=47)

# COMMAND ----------

# DBTITLE 1,Datasetsã‹ã‚‰1ãƒ¬ã‚³ãƒ¼ãƒ‰å–ã‚Šå‡ºã—ã€ä¸­èº«ã‚’è¦‹ã¦ã¿ã‚‹
train_dataset[0]

# COMMAND ----------

# MAGIC %md
# MAGIC ## 1-3. ãƒ¢ãƒ‡ãƒ«ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°

# COMMAND ----------

# MAGIC %md
# MAGIC ãƒ­ã‚°ã«è¨˜éŒ²ã™ã‚‹è©•ä¾¡æŒ‡æ¨™ã‚’å®šç¾©ã—ã¾ã™ã€‚ä»Šå›ã¯Accuracyã‚’è¨˜éŒ²ã—ã¾ã™ã€‚æå¤±ï¼ˆLossï¼‰ã¯è¨­å®šã›ãšã¨ã‚‚è‡ªå‹•ã§ãƒ­ã‚°ã«è¨˜éŒ²ã•ã‚Œã¾ã™ã€‚

# COMMAND ----------

import numpy as np
import evaluate

metric = evaluate.load("accuracy")
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    return metric.compute(predictions=predictions, references=labels)

# COMMAND ----------

# MAGIC %md
# MAGIC å­¦ç¿’ç”¨ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã¯ã»ã¼ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ä½¿ç”¨ã—ã¾ã™ãŒã€Epochæ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯3ï¼‰ã¨ãƒãƒƒãƒã‚µã‚¤ã‚ºï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯8ï¼‰ã®ã¿ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‹ã‚‰å¤‰æ›´ã—ã¾ã™ã€‚ä»–ã«ã‚‚å¿…è¦ã«å¿œã˜ã¦å­¦ç¿’ç‡ãªã©ã®å¤šãã®å­¦ç¿’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¨­å®šã§ãã¾ã™ã€‚è©³ç´°ã¯
# MAGIC [transformersãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments) ã‚’å‚ç…§ãã ã•ã„ã€‚

# COMMAND ----------

from transformers import TrainingArguments

training_output_dir = f"{tutorial_path}/bert_trainer"
training_args = TrainingArguments(
  output_dir=training_output_dir, 
  logging_dir = f"{tutorial_path}/logs",    # TensorBoardç”¨ã«ãƒ­ã‚°ã‚’è¨˜éŒ²ã™ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
  evaluation_strategy="epoch",
  num_train_epochs=5)

training_args.set_dataloader(train_batch_size=12, eval_batch_size=32)

print(f"å­¦ç¿’æ™‚ã®ãƒãƒƒãƒã‚µã‚¤ã‚ºã¯ã€€{training_args.per_device_train_batch_size}ã€æ¤œè¨¼æ™‚ã®ãƒãƒƒãƒã‚µã‚¤ã‚ºã¯ã€€{training_args.per_device_eval_batch_size}ã€€ã§ã™ã€‚")


# COMMAND ----------

# MAGIC %md
# MAGIC ãƒ©ãƒ™ãƒ«ãƒãƒƒãƒ”ãƒ³ã‚°ã¨ã‚¯ãƒ©ã‚¹æ•°ã‚’æŒ‡å®šã—ã¦ã€ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰å­¦ç¿’ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆã—ã¾ã™ã€‚

# COMMAND ----------

from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained(
  base_model, 
  num_labels=len(category_list), 
  label2id=label2id, 
  id2label=id2label)

# COMMAND ----------

# MAGIC %md
# MAGIC [data collator](https://huggingface.co/docs/transformers/main_classes/data_collator)ã‚’ä½¿ã†ã“ã¨ã§ã€è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å…¥åŠ›ã‚’ãƒãƒƒãƒåŒ–ã™ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚`DataCollatorWithPadding`ã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ä½¿ç”¨ã™ã‚‹ã¨ã€ãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡ã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ€§èƒ½ãŒè‰¯ããªã‚‹ã€‚

# COMMAND ----------

from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer)

# COMMAND ----------

# MAGIC %md
# MAGIC ä¸Šè¨˜ã§ä½œæˆã—ãŸãƒ¢ãƒ‡ãƒ«ã€å¼•æ•°ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€ç…§åˆå™¨ã€ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ç”¨ã„ã¦ã€ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’æ§‹ç¯‰ã—ã¾ã™ã€‚

# COMMAND ----------

from transformers import Trainer

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    compute_metrics=compute_metrics,
    data_collator=data_collator,
)

# COMMAND ----------

# MAGIC %md
# MAGIC ãƒ¢ãƒ‡ãƒ«ã‚’ã‚µãƒ¼ãƒ“ãƒ³ã‚°ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã¨ã—ã¦ãƒ‡ãƒ—ãƒ­ã‚¤ã™ã‚‹ãŸã‚ã®MLFlowç™»éŒ²ç”¨ãƒ©ãƒƒãƒ‘ãƒ¼ã‚¯ãƒ©ã‚¹ã§ã™ã€‚
# MAGIC æœ¬æ¥ã¯MLFlowã®Transformerãƒ•ãƒ¬ãƒ¼ãƒãƒ¼ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã§ã‚ˆã‚Šç°¡å˜ã«ç™»éŒ²ã§ãã‚‹ã®ã§ã™ãŒã€ãã‚Œã ã¨ã‚µãƒ¼ãƒ“ãƒ³ã‚°æ™‚ã«GPUãŒä½¿ç”¨ã•ã‚Œãªã„ã‚±ãƒ¼ã‚¹ãŒã‚ã‚‹ãŸã‚ã€GPUä½¿ç”¨ã‚’ã™ã‚‹ã«ã¯ã“ã®å®Ÿè£…ãŒå¿…è¦ã§ã™ã€‚

# COMMAND ----------

import mlflow
import torch

class TextClassificationPipelineModel(mlflow.pyfunc.PythonModel):
  
  def __init__(self, model, tokenizer):
    device = 0 if torch.cuda.is_available() else -1
    self.pipeline = pipeline(
      "text-classification", 
      model=model, 
      tokenizer=tokenizer,
      batch_size=1,
      device=device)
    self.tokenizer = tokenizer
    
  def predict(self, context, model_input): 
    messages = model_input["text"].to_list()
    answers = self.pipeline(messages, max_length=max_length, padding='max_length', truncation=True)

    label_list = []
    score_list = []
    for answer in answers:
      label_list.append(answer['label'])
      score_list.append(str(answer['score']))

    return {"label": label_list, "score": score_list}

# COMMAND ----------

# MAGIC %md
# MAGIC ãƒ¢ãƒ‡ãƒ«ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã€ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã¨çµæœã‚’ MLflow ã«è¨˜éŒ²ã—ã¾ã™ã€‚
# MAGIC MLFlowã®Transformerãƒ•ãƒ¬ãƒ¼ãƒãƒ¼ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã§ã€ç°¡å˜ã«è¨˜éŒ²ã§ãã¾ã™ã€‚

# COMMAND ----------

from transformers import pipeline

from mlflow.models.signature import ModelSignature
from mlflow.types import DataType, Schema, ColSpec

model_output_dir = f"{tutorial_path}/trained_model"
pipeline_output_dir = f"{tutorial_path}/trained_pipeline"

with mlflow.start_run() as run:
  
  # å­¦ç¿’é–‹å§‹ã€‚å­¦ç¿’ã®ãƒ¡ãƒˆãƒªãƒƒã‚¯ãŒè‡ªå‹•çš„ã«MLFLowã«ãƒ­ã‚®ãƒ³ã‚°ã•ã‚Œã‚‹
  trainer.train()

  # å­¦ç¿’çµ‚äº†å¾Œã«ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜
  trainer.save_model(model_output_dir)
  
  # å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åŒ–ã—ã¦ã€æ›´ã«ä¿å­˜ã€‚
  bert = AutoModelForSequenceClassification.from_pretrained(model_output_dir)

  # pipe = pipeline(
  #   "text-classification", 
  #   model=bert, 
  #   batch_size=1, 
  #   tokenizer=tokenizer,
  #   device=0)
  # pipe.save_pretrained(pipeline_output_dir)
  
  #######################################
  # CPUã®ã¿ã§ã®ã‚µãƒ¼ãƒ“ãƒ³ã‚°ã§è‰¯ã‘ã‚Œã°ã“ã¡ã‚‰ã§ã‚‚å¯
  #######################################
  # # MLFlow Trackingã«ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’è¨˜éŒ²ã™ã‚‹ã€‚
  # mlflow.transformers.log_model(
  #   transformers_model=pipe, 
  #   artifact_path=model_artifact_path+"_CPU", 
  #   input_example=["ã“ã‚Œã¯ã‚µãƒ³ãƒ—ãƒ«ï¼‘ã§ã™ã€‚", "ã“ã‚Œã¯ã‚µãƒ³ãƒ—ãƒ«ï¼’ã§ã™ã€‚"],
  #   pip_requirements=["torch", "transformers", "accelerate", "sentencepiece", "datasets", "evaluate", "fugashi", "ipadic", "unidic-lite"],
  #   # registered_model_name=registered_model_name,
  #   model_config={ 
  #     "max_length": max_length, 
  #     "padding": "max_length", 
  #     "truncation": True 
  #   }
  # )

  # ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã®å…¥åŠ›ã¨å‡ºåŠ›ã®ã‚¹ã‚­ãƒ¼ãƒã‚’å®šç¾©
  input_schema = Schema([ColSpec(DataType.string, "text")])
  output_schema = Schema([ColSpec(DataType.string, "label"), ColSpec(DataType.double, "score")])
  signature = ModelSignature(inputs=input_schema, outputs=output_schema)

  # å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®ã‚µãƒ³ãƒ—ãƒ«ã‚’ç”¨æ„
  input_example = pd.DataFrame({"text": ["ã“ã‚Œã¯ã‚µãƒ³ãƒ—ãƒ«ï¼‘ã§ã™ã€‚", "ã“ã‚Œã¯ã‚µãƒ³ãƒ—ãƒ«ï¼’ã§ã™ã€‚"]})
  
  # ãƒ¢ãƒ‡ãƒ«ã‚’MLFlow Trackingã«è¨˜éŒ²
  mlflow.pyfunc.log_model(
      artifact_path=model_artifact_path,
      python_model=TextClassificationPipelineModel(bert, tokenizer),
      pip_requirements=["torch", "transformers", "accelerate", "sentencepiece", "datasets", "evaluate", "fugashi", "unidic-lite"],
      input_example=input_example,
      signature=signature
  )

print(f"ãƒ¢ãƒ‡ãƒ«ã¯MLFlowå®Ÿé¨“ã®Run(ID:{run.info.run_id})ã«è¨˜éŒ²ã•ã‚Œã¾ã—ãŸã€‚ã“ã®IDã‚’è¨˜éŒ²ã—ã¦ãŠã„ã¦ãã ã•ã„ã€‚")

# COMMAND ----------

# DBTITLE 1,TensorBoardç”¨ã«ãƒ­ã‚°ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ¼ã‚’æŒ‡å®š
# MAGIC %load_ext tensorboard
# MAGIC experiment_log_dir = f"{tutorial_path}/logs"

# COMMAND ----------

# DBTITLE 1,TensorBoardèµ·å‹•
# MAGIC %tensorboard --logdir $experiment_log_dir

# COMMAND ----------

# MAGIC %md
# MAGIC ## 1-4. å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã§æ¨è«–

# COMMAND ----------

# DBTITLE 1,ãƒ†ã‚¹ãƒˆç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
# ãƒ†ã‚¹ãƒˆç”¨ã®æ–‡ç« ãƒ‡ãƒ¼ã‚¿
inputs = [
    """
ã‚ªãƒ¼ã‚·ãƒ£ãƒ³ã‚·ãƒ†ã‚£ã«æ‰€å±ã™ã‚‹ãƒã‚ªãƒ©ãƒ³ãƒ‰ä»£è¡¨FWã‚¢ãƒ¬ãƒƒã‚¯ã‚¹ãƒ»ã‚¹ã‚¿ãƒ¼ãƒãƒ³ãŒã€å…¨ä½“ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã«å¾©å¸°ã—ãŸã€‚13æ—¥ã€ã‚¯ãƒ©ãƒ–å…¬å¼ã‚µã‚¤ãƒˆãŒä¼ãˆã¦ã„ã‚‹ã€‚
ã‚ªãƒ¼ã‚·ãƒ£ãƒ³ã‚·ãƒ†ã‚£ã‚’é›¢ã‚Œã€ãƒã‚ªãƒ©ãƒ³ãƒ‰ä»£è¡¨ã§ã®ãƒŸã‚¹ãƒ†ã‚£ãƒƒã‚¯ã‚³ãƒ³ãƒãƒãƒ³ãƒ„ã‚«ãƒƒãƒ—ã«å‚æˆ¦ã—ã¦ã„ãŸã‚¹ã‚¿ãƒ¼ãƒãƒ³ã¯ã€å…ˆæœˆ18æ—¥ã«è¡Œã‚ã‚ŒãŸç¬¬2ç¯€ã®ã‚µãƒ³ãƒ©ã‚¤ãƒˆä»£è¡¨æˆ¦ï¼ˆâ–³2ï¼2ï¼‰ã§å·¦å¤ªã‚‚ã‚‚è£ã‚’è² å‚·ã—ã¦ã—ã¾ã„ã€å‰åŠã‚¢ãƒ‡ã‚£ã‚·ãƒ§ãƒŠãƒ«ã‚¿ã‚¤ãƒ ã«é€”ä¸­äº¤ä»£ã‚’ä½™å„€ãªãã•ã‚Œã€ãƒãƒ¼ãƒ ã‚’é›¢è„±ã—ã¦ã‚ªãƒ¼ã‚·ãƒ£ãƒ³ã‚·ãƒ†ã‚£ã«å¾©å¸°ã—ã¦ã„ãŸã€‚
    """,
    """
M3ã‚·ãƒªãƒ¼ã‚ºã¯ã€8ã‚³ã‚¢ã®CPUã¨10ã‚³ã‚¢ã®GPUã‚’å‚™ãˆã€æœ€å¤§ã§24GBã®çµ±åˆãƒ¡ãƒ¢ãƒªã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã™ã€‚M3 Proãƒ¢ãƒ‡ãƒ«ã§ã¯ã€CPUã‚’æœ€å¤§12ã‚³ã‚¢ã€GPUã‚’æœ€å¤§18ã‚³ã‚¢ã¾ã§é¸æŠå¯èƒ½ã§ã€çµ±åˆãƒ¡ãƒ¢ãƒªã¯æœ€å¤§ã§36GBã§ã™ã€‚ä¸€æ–¹ã€M3 Maxã§ã¯CPUã‚’æœ€å¤§ã§16ã‚³ã‚¢ã€GPUã‚’æœ€å¤§ã§40ã‚³ã‚¢ã€çµ±åˆãƒ¡ãƒ¢ãƒªã‚’æœ€å¤§ã§128GBã¾ã§é¸æŠã§ãã¾ã™ã€‚
ã“ã‚Œã‚‰å…¨ãƒ¢ãƒ‡ãƒ«ã«å…±é€šã—ã¦ã€AV1ã‚³ãƒ¼ãƒ‡ãƒƒã‚¯ã®ãƒ‡ã‚³ãƒ¼ãƒ‰ã‚’ã‚µãƒãƒ¼ãƒˆã™ã‚‹å†…è”µãƒ¡ãƒ‡ã‚£ã‚¢ã‚¨ãƒ³ã‚¸ãƒ³ã‚’æ­è¼‰ã—ã¦ãŠã‚Šã€HEVCã€H.264ã€ProResãªã©ã®æ§˜ã€…ãªãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã®å†ç”ŸãŒå¯èƒ½ã§ã™ã€‚ã•ã‚‰ã«ã€Dolby Visionã€HDR 10ã€HLGã¨ã„ã£ãŸé«˜ãƒ€ã‚¤ãƒŠãƒŸãƒƒã‚¯ãƒ¬ãƒ³ã‚¸ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«ã‚‚å¯¾å¿œã—ã¦ã„ã¾ã™ã€‚
    """,
    """
ã€€ã—ã‹ã—ã€æ‹æ„›ã«ãŠã„ã¦å—å‹•çš„ãªå§¿å‹¢ã‹ã‚‰å§‹ã‚ã‚‹ã“ã¨ãŒã€æ¥½ã ã¨æ„Ÿã˜ã¦ã—ã¾ã†ã¨ã€é–¢ä¿‚ãŒå§‹ã¾ã£ã¦ã‹ã‚‰å¹³ç­‰ãªä¿¡é ¼é–¢ä¿‚ã‚’æ§‹ç¯‰ã™ã‚‹ã®ãŒé›£ã—ããªã‚ŠãŒã¡ã§ã™ã€‚ç”·æ€§ãŒãƒªãƒ¼ãƒ‰ã—ã€å¥³æ€§ãŒãã‚Œã«å¾“ã†ã¨ã„ã†é–¢ä¿‚ãŒå®šç€ã™ã‚‹ã¨ã€å¥³æ€§ã¯å¾ã€…ã«ã€Œå«Œã‚ã‚ŒãŸããªã„ã€ã¨ã„ã†æ€ã„ã‹ã‚‰è‡ªã‚‰ã®æ„è¦‹ã‚’è¿°ã¹ã«ãããªã‚Šã¾ã™ã€‚ãã—ã¦ã€å ´åˆã«ã‚ˆã£ã¦ã¯ã€ç”·æ€§ãŒæ”¯é…çš„ãªæ…‹åº¦ã‚’ã¨ã‚‹ã‚ˆã†ã«ãªã‚Šã€ãã®ã‚ˆã†ãªé–¢ä¿‚ã‹ã‚‰æŠœã‘å‡ºã™ã®ãŒå›°é›£ã«ãªã‚‹ã“ã¨ã‚‚ã‚ã‚‹ã®ã§ã™ã€‚
    """,
    """
21ä¸–ç´€ãƒ”ã‚¯ãƒãƒ£ãƒ¼ã‚ºãŒã€äººæ°—SFã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã€ãƒãƒ³ã‚¿ãƒ¼ã‚ºã€ã‚·ãƒªãƒ¼ã‚ºã®æ–°ä½œæ˜ ç”»ã€ãƒ€ãƒ¼ã‚¯ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ï¼ˆåŸé¡Œï¼‰ / Darkfieldã€ã®è£½ä½œã‚’é€²ã‚ã¦ã„ã‚‹ã¨ã€The Global Film Gazette ãªã©ãŒå ±ã˜ãŸã€‚
ã€€ç›£ç£ã¯å‰ä½œã€ãƒãƒ³ã‚¿ãƒ¼ã‚ºï¼šã‚¶ãƒ»ã‚¯ã‚¨ã‚¹ãƒˆã€ï¼ˆ2022ï¼‰ã¨åŒã˜ãã‚¸ãƒ§ãƒ³ãƒ»ãƒ‰ãƒ¼ãƒ´ã‚¡ãƒ¼ãŒå‹™ã‚ã‚‹ãŒã€åŒã‚µã‚¤ãƒˆã«ã‚ˆã‚‹ã¨ã€æ–°ä½œã¯ã€ã‚¶ãƒ»ã‚¯ã‚¨ã‚¹ãƒˆã€ã®ç¶šç·¨ã«ã¯ãªã‚‰ãªã„ã¨ã®ã“ã¨ã€‚è©³ç´°ã¯ä¸æ˜ã ãŒã€æœªæ¥ã‚’èˆå°ã«ã€ã€ã‚¶ãƒ»ã‚¯ã‚¨ã‚¹ãƒˆã€ã¨åŒã˜ãå¥³æ€§ãŒä¸»äººå…¬ã«ãªã‚‹ã€‚
    """
]

# COMMAND ----------

# DBTITLE 1,å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã£ã¦ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ä½œæˆã—æ¨è«–å®Ÿè¡Œ
trained_pipe = pipeline(
    "text-classification", 
    model=AutoModelForSequenceClassification.from_pretrained(model_output_dir), 
    batch_size=4, 
    top_k=3,
    tokenizer=tokenizer,
    device=0)
res = trained_pipe(inputs)
res

# COMMAND ----------

# MAGIC %md
# MAGIC ## ãŠç–²ã‚Œæ§˜ã§ã—ãŸï¼
